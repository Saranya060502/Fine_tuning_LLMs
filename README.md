# Fine_tuning_LLMs
This project showcases my implementation of fine-tuning a pre-trained **BERT** model (110M parameters) to classify phishing URLs, based on insights from a detailed blog I studied. I replicated the process using the Hugging Face Transformers library, freezing most of BERT’s parameters to reduce computational costs while fine-tuning the classification head and pooling layers. Using a dataset of 3,000 labeled URL-text pairs, I applied tokenization, truncation, and dynamic padding, then trained the model over 10 epochs with a learning rate of `2e-4`. The fine-tuned model achieved **89% accuracy** and an **AUC of 94.6%** on an independent validation set, validating the effectiveness of the method.  

This replication deepened my understanding of fine-tuning pre-trained models for specific tasks while optimizing resource usage. The project highlights the flexibility of BERT and the Hugging Face ecosystem for tasks like phishing detection. Additionally, I explored compression techniques suggested in the blog to further reduce the model’s memory footprint, making it suitable for deployment on hardware with limited resources. This approach can be readily adapted to other text classification problems, such as spam detection or sentiment analysis.
